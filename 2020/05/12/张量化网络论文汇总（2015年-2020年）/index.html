<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>张量化网络论文汇总（2015年-2020年） | 一个努力吃胖的小瘦子的博客</title><meta name="description" content="用张量分解的方法压缩神经网络模型这一方向还有哪些坑可以挖呢？这是我研究生以来一直研究的问题。看了很多论文，感觉大体可以分为两条line: (一)基于低秩近似的张量分解方法也就是对原有的模型参数做低秩张量分解，用分解后得到的因子替换原有的大张量。这一过程后通常还需要一个fine-tune的过程。其中的难点就是怎么从大的张量中保留最有价值的参数留下来，作为一个很好的初始参数值。 (二)张量化网络用张量"><meta name="keywords" content="神经网络,张量化"><meta name="author" content="一个努力吃胖的小瘦子"><meta name="copyright" content="一个努力吃胖的小瘦子"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="dns-prefetch" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://hm.baidu.com"/><link rel="dns-prefetch" href="https://hm.baidu.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="dns-prefetch" href="https://fonts.googleapis.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="dns-prefetch" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="张量化网络论文汇总（2015年-2020年）"><meta name="twitter:description" content="用张量分解的方法压缩神经网络模型这一方向还有哪些坑可以挖呢？这是我研究生以来一直研究的问题。看了很多论文，感觉大体可以分为两条line: (一)基于低秩近似的张量分解方法也就是对原有的模型参数做低秩张量分解，用分解后得到的因子替换原有的大张量。这一过程后通常还需要一个fine-tune的过程。其中的难点就是怎么从大的张量中保留最有价值的参数留下来，作为一个很好的初始参数值。 (二)张量化网络用张量"><meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="og:type" content="article"><meta property="og:title" content="张量化网络论文汇总（2015年-2020年）"><meta property="og:url" content="https://zhaoyibo61.github.io/2020/05/12/%E5%BC%A0%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB%EF%BC%882015%E5%B9%B4-2020%E5%B9%B4%EF%BC%89/"><meta property="og:site_name" content="一个努力吃胖的小瘦子的博客"><meta property="og:description" content="用张量分解的方法压缩神经网络模型这一方向还有哪些坑可以挖呢？这是我研究生以来一直研究的问题。看了很多论文，感觉大体可以分为两条line: (一)基于低秩近似的张量分解方法也就是对原有的模型参数做低秩张量分解，用分解后得到的因子替换原有的大张量。这一过程后通常还需要一个fine-tune的过程。其中的难点就是怎么从大的张量中保留最有价值的参数留下来，作为一个很好的初始参数值。 (二)张量化网络用张量"><meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2020-05-12T07:55:00.000Z"><meta property="article:modified_time" content="2020-05-13T07:39:18.475Z"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://zhaoyibo61.github.io/2020/05/12/%E5%BC%A0%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB%EF%BC%882015%E5%B9%B4-2020%E5%B9%B4%EF%BC%89/"><link rel="prev" title="一步步带你免费使用Tesla K80 GPU！" href="https://zhaoyibo61.github.io/2020/05/12/%E4%B8%80%E6%AD%A5%E6%AD%A5%E5%B8%A6%E4%BD%A0%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8Tesla%20K80%20GPU%EF%BC%81/"><link rel="next" title="深度神经网络模型训练时GPU显存不足怎么办？" href="https://zhaoyibo61.github.io/2020/05/12/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%97%B6GPU%E6%98%BE%E5%AD%98%E4%B8%8D%E8%B6%B3%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F/"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?a2e0ebebca52a3d594f4581c74967d36";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: true
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: false  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="一个努力吃胖的小瘦子的博客" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><canvas class="fireworks"></canvas><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/QQ.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">17</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">35</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">3</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 菜单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">一个努力吃胖的小瘦子的博客</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 菜单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">张量化网络论文汇总（2015年-2020年）</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-05-12 15:55:00"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-05-12</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-05-13 15:39:18"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-05-13</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">982</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 3 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p><strong>用张量分解的方法压缩神经网络模型这一方向还有哪些坑可以挖呢？这是我研究生以来一直研究的问题。看了很多论文，感觉大体可以分为两条line:</strong></p>
<p><strong>(一)基于低秩近似的张量分解方法</strong><br>也就是对原有的模型参数做低秩张量分解，用分解后得到的因子替换原有的大张量。这一过程后通常还需要一个fine-tune的过程。其中的难点就是怎么从大的张量中保留最有价值的参数留下来，作为一个很好的初始参数值。</p>
<p><strong>(二)张量化网络</strong><br>用张量分解得到的因子重新定义新的网络结构，新的张量计算方法代替原来的卷积操作或全连接层的矩阵运算。这一方法姑且就叫张量化网络吧，下面是该方向的论文汇总，按时间顺序排列。</p>
<p>1.《Tensorizing Neural Networks》(NIPS2015)<br>2.《Ultimate tensorization compressing convolutional and FC layers alike》（NIPS2016 workshop）<br>3.《Compressing recurrent neural network with tensor train 》(IJCNN2017)<br>4.《Tensor-Train Recurrent Neural Networks for Video Classification》(ICML2017)<br>5.《Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition》（CVPR2018）<br>6.《Sharing Residual Units Through Collective Tensor Factorization》(IJCAI2018)<br>7.《Tensor Regression Networks》（2018）<br>8.《Tensor Regression Networks with various Low-Rank Tensor Approximations》（2018）<br>9.《Wide Compression：Tensor Ring Nets 》(CVPR2018)<br>10.《Compressing Recurrent Neural Networks with Tensor Ring for Action Recognition》（AAAI2019）<br>11.《Bayesian Tensorized Neural Networks with Automatic Rank Selection》（2019）<br>12.《Robust deep networks with randomized tensor regression layers》（2019）<br>13.《Compressing Deep Neural Networks via AdaptiveDimension Adjustment Tucker Decomposition》（2019）<br>14.《Compression and Interpretability of Deep Neural Networks via Tucker Tensor Layer：From First Principles to Tensor Valued Back-Propagation》（2020）<br>从时间顺序上来看这条line的发展脉络还蛮清晰的</p>
<p>1,2是这个方向的开篇之作，用tensor train分解方法分别重新定义了全连接层和卷积层，都是先将它们reshape成更高维的张量。</p>
<p>3,4都是基于1做的，方法基本相同，只不过扩展到了RNN网络中。</p>
<p>5同样是对RNN网络做压缩，只不过换了一种分解方法block-term分解。</p>
<p>6和之前的思想不是十分相同，它利用了block-term分解后的参数形式对各种resnet残差网络用一个框架表示。tucker分解后的参数可以转化为1<em>1,3</em>3,1*1的卷积层，早在2016年就有。而block-term分解是tucker分解的一种泛化形式，可以看做多个tucker的集成。</p>
<p>7,8是一组，它们针对的神经网络中的最后一层分类层。</p>
<p>9用了一种新提出的tensor ring分解方法，对卷积和全连接层都进行了重新的表示。</p>
<p>10和9的方法一样，只不过任务不一样，不过10这篇论文写的真好，一读就懂。</p>
<p>11用了贝叶斯推理法，看不懂。</p>
<p>12是7,8的后续工作，将随机的思想添加了进来。</p>
<p>13主要研究的是分解前将参数reshape成哪种维度比较好，并且core tensor的维度怎么取比较好。</p>
<p>14试图通过张量对神经网络模型的可解释性进行探索，主要通过对tucker分解后的因子求偏导，比较大小。</p>
<p><strong>总结下来，其实并没有很大的飞跃，基本就是有了新的更好的张量分解方法就将其应用在网络的参数上；或是应用在不同的神经网络模型上；或是研究一下分解之前把全连接层的矩阵和卷积层的四维张量reshape成多大维度的张量。难点主要是秩的选择。因为张量分解中的秩决定分解后因子的维度，也就是决定了张量化后的网络模型的结构。而目前几乎所有的工作都是凭经验指定的或者通过实验挑选最优的。</strong></p>
<p>未来能刨的坑还有很多！</p>
<p>以上转载自博客 <a href="https://blog.csdn.net/sinat_38856440/article/details/104450094/" target="_blank" rel="noopener">link</a>.</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">一个努力吃胖的小瘦子</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhaoyibo61.github.io/2020/05/12/%E5%BC%A0%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB%EF%BC%882015%E5%B9%B4-2020%E5%B9%B4%EF%BC%89/">https://zhaoyibo61.github.io/2020/05/12/%E5%BC%A0%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB%EF%BC%882015%E5%B9%B4-2020%E5%B9%B4%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhaoyibo61.github.io" target="_blank">一个努力吃胖的小瘦子的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/%E5%BC%A0%E9%87%8F%E5%8C%96/">张量化</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/img/wechat.jpg" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="post-qr-code__img" src="/img/alipay.jpg" alt="支付宝"/><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/05/12/%E4%B8%80%E6%AD%A5%E6%AD%A5%E5%B8%A6%E4%BD%A0%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8Tesla%20K80%20GPU%EF%BC%81/"><img class="prev_cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">一步步带你免费使用Tesla K80 GPU！</div></div></a></div><div class="next-post pull_right"><a href="/2020/05/12/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%97%B6GPU%E6%98%BE%E5%AD%98%E4%B8%8D%E8%B6%B3%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F/"><img class="next_cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">深度神经网络模型训练时GPU显存不足怎么办？</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/05/12/TensorLy-神经网络张量库/" title="TensorLy-神经网络张量库"><img class="relatedPosts_cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-12</div><div class="relatedPosts_title">TensorLy-神经网络张量库</div></div></a></div></div><div class="clear_both"></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 一个努力吃胖的小瘦子</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi，我是一个努力吃胖的小瘦子，欢迎来到我的个人博客！</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script id="ribbon_piao" mobile="false" src="/js/third-party/piao.js"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="/js/third-party/canvas-nest.js"></script><script src="/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
document.body.addEventListener('input', POWERMODE);
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="/js/third-party/click_heart.js"></script><script src="https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js"></script><script>document.addEventListener('DOMContentLoaded', function() {
      pangu.spacingElementById('content-inner')
})</script><script src="/js/search/local-search.js"></script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script></body></html>